import numpy as np
import pandas as pd
import streamlit as st
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split


def preprocesamiento_titanic():
    st.title("üö¢ Ejercicio 1: Preprocesamiento del Dataset Titanic (Versi√≥n completa paso a paso)")

    # ============================================
    # 1Ô∏è‚É£ CARGA DEL DATASET
    # ============================================
    st.subheader("1Ô∏è‚É£ Cargar el dataset con pandas")
    st.write("Leemos el archivo `Titanic-Dataset.csv` desde la carpeta `data/`.")
    dataset = pd.read_csv("data/Titanic-Dataset.csv")

    st.write("**Vista previa del dataset original:**")
    st.dataframe(dataset.head())

    # ============================================
    # 2Ô∏è‚É£ SELECCIONAR COLUMNAS RELEVANTES
    # ============================================
    st.subheader("2Ô∏è‚É£ Seleccionar columnas relevantes")
    st.write("""
    Mantenemos solo las columnas relevantes para el modelo:
    `Survived`, `Pclass`, `Sex`, `Age`, `SibSp`, `Parch`, `Fare`, `Embarked`.
    """)
    dataset = dataset[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]
    st.dataframe(dataset.head())

    # ============================================
    # 3Ô∏è‚É£ MATRIZ DE VARIABLES (X) Y VECTOR (y)
    # ============================================
    st.subheader("3Ô∏è‚É£ Definici√≥n de variables independientes y dependiente")
    X = dataset.iloc[:, 1:].values
    y = dataset.iloc[:, 0].values

    st.write("**Primeras filas de X (variables predictoras):**")
    st.dataframe(pd.DataFrame(X, columns=dataset.columns[1:]).head())
    st.write("**Primeras filas de y (variable objetivo):**")
    st.dataframe(pd.DataFrame(y, columns=["Survived"]).head())

    # ============================================
    # 4Ô∏è‚É£ TRATAMIENTO DE VALORES NULOS
    # ============================================
    st.subheader("4Ô∏è‚É£ Tratamiento de valores faltantes")
    st.write("""
    Reemplazamos los valores faltantes:
    - **Age** ‚Üí media  
    - **Embarked** ‚Üí moda
    """)
    st.write("**Valores nulos antes:**")
    st.dataframe(dataset.isnull().sum())

    # Age ‚Üí media
    imputer = SimpleImputer(missing_values=np.nan, strategy="mean")
    imputer = imputer.fit(X[:, [2]])  # Columna 'Age'
    X[:, [2]] = imputer.transform(X[:, [2]])

    # Embarked ‚Üí moda
    embarked_col = pd.Series(X[:, 6])
    moda_embarked = embarked_col.mode()[0]
    embarked_col.fillna(moda_embarked, inplace=True)
    X[:, 6] = embarked_col

    st.write("**Valores nulos despu√©s (verificaci√≥n):**")
    df_temp = pd.DataFrame(X, columns=dataset.columns[1:])
    st.dataframe(df_temp.isnull().sum())

    st.write("**Dataset tras reemplazar valores faltantes:**")
    st.dataframe(df_temp.head())

    # ============================================
    # 5Ô∏è‚É£ CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS
    # ============================================
    st.subheader("5Ô∏è‚É£ Codificaci√≥n de variables categ√≥ricas")
    st.write("""
    - `Sex`: con LabelEncoder (0 = male, 1 = female)  
    - `Embarked`: con OneHotEncoder
    """)

    # Sex
    le_sex = LabelEncoder()
    X[:, 1] = le_sex.fit_transform(X[:, 1])

    st.write("**Despu√©s de codificar 'Sex':**")
    st.dataframe(pd.DataFrame(X, columns=dataset.columns[1:]).head())

    # Embarked ‚Üí OneHotEncoder
    ct = ColumnTransformer(
        [('encoder', OneHotEncoder(categories='auto'), [6])],
        remainder='passthrough'
    )
    X = np.array(ct.fit_transform(X), dtype=np.float64)

    st.write("**Despu√©s de aplicar OneHotEncoder a 'Embarked':**")
    st.dataframe(pd.DataFrame(X).head())
    st.markdown("""
    Nota: Las nuevas columnas creadas por OneHotEncoder para 'Embarked' son:
    - Embarked_C
    - Embarked_Q
    - Embarked_S 
                
    Lo que hace OneHotEncoder es crear columnas binarias para cada categor√≠a :D, es m√©todo es el m√°s recomendado.
    """)
    # ============================================
    # 6Ô∏è‚É£ DIVISI√ìN EN TRAIN Y TEST
    # ============================================
    st.subheader("6Ô∏è‚É£ Divisi√≥n del dataset en entrenamiento y prueba")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    st.write(f"üîπ X_train: {X_train.shape}")
    st.write(f"üîπ X_test: {X_test.shape}")
    st.write(f"üîπ y_train: {y_train.shape}")
    st.write(f"üîπ y_test: {y_test.shape}")

    # Mostrar una parte del conjunto de entrenamiento
    st.write("**Primeras filas de X_train:**")
    st.dataframe(pd.DataFrame(X_train).head())

    # ============================================
    # 7Ô∏è‚É£ ESCALADO DE VARIABLES NUM√âRICAS
    # ============================================
    st.subheader("7Ô∏è‚É£ Escalado de variables num√©ricas")
    st.write("""
    Se aplica `StandardScaler` para que todas las variables num√©ricas
    est√©n en una escala comparable.
    """)

    sc = StandardScaler()
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)

    st.write("**Primeras filas de X_train escalado:**")
    st.dataframe(pd.DataFrame(X_train).head())

    # ============================================
    # 8Ô∏è‚É£ CONCLUSI√ìN
    # ============================================
    st.subheader("8Ô∏è‚É£ Conclusi√≥n del preprocesamiento")
    st.write("""
    ‚úîÔ∏è Cargamos el dataset  
    ‚úîÔ∏è Seleccionamos columnas relevantes  
    ‚úîÔ∏è Tratamos valores faltantes  
    ‚úîÔ∏è Codificamos variables categ√≥ricas  
    ‚úîÔ∏è Dividimos en entrenamiento/prueba  
    ‚úîÔ∏è Escalamos las variables num√©ricas  
    """)
    st.success("üéØ Preprocesamiento completado con √©xito.")
    return X_train, X_test, y_train, y_test
